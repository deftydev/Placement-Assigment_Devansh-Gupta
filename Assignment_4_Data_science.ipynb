{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_-bqL1zYSRl"
      },
      "outputs": [],
      "source": [
        "General Linear Model:\n",
        "\n",
        "1. What is the purpose of the General Linear Model (GLM)?\n",
        "when there is a linear relationship betwen independent features and dependent feature then there are GLM which can predict the dependent features.\n",
        "\n",
        "2. What are the key assumptions of the General Linear Model?\n",
        "Linearity, normality, exogeneity(error and independent feature should be not correlated), homdascity(error terms should be of constant variance)\n",
        "No Multicollinearity\n",
        "\n",
        "3. How do you interpret the coefficients in a GLM?\n",
        "m is the slope\n",
        "c is the coeffiecient\n",
        "l=LinearRegression\n",
        "l.fit(train,test)\n",
        "m is by l._inetrcept\n",
        "c is by l._coef\n",
        "\n",
        "4. What is the difference between a univariate and multivariate GLM?\n",
        "Univariate GLM:\n",
        "\n",
        "In a univariate GLM, there is a single dependent variable or outcome variable.\n",
        "The analysis focuses on modeling and understanding the relationship between this single dependent variable and one or more independent variables.\n",
        "The GLM is used to estimate the effect of the independent variables on the mean or distribution of the single dependent variable.\n",
        "Multivariate GLM:\n",
        "\n",
        "In a multivariate GLM, there are multiple dependent variables or outcome variables.\n",
        "The analysis simultaneously considers the relationships between multiple dependent variables and one or more independent variables.\n",
        "\n",
        "\n",
        "6. How do you handle categorical predictors in a GLM?\n",
        "by using sigma loss function\n",
        "logistic regression\n",
        "\n",
        "\n",
        "8. How do you test the significance of predictors in a GLM?\n",
        "Hypothesis Testing with t-tests:\n",
        "\n",
        "The t-test is commonly used to test the significance of individual predictor coefficients in a GLM.\n",
        "The null hypothesis (H0) states that the coefficient for a predictor variable is zero, implying no effect of that predictor on the response variable.\n",
        "The alternative hypothesis (Ha) states that the coefficient is non-zero, indicating a significant effect of the predictor.\n",
        "The t-test assesses the ratio of the estimated coefficient to its standard error, comparing it to a t-distribution with degrees\n",
        "of freedom equal to the residual degrees of freedom.\n",
        "If the computed t-value exceeds a critical value at a chosen significance level (e.g., 0.05), the null hypothesis is rejected, indicating a\n",
        "significant predictor.\n",
        "\n",
        "9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?\n",
        "In a General Linear Model (GLM), the concept of sums of squares (SS) is used to partition the total variability in the\n",
        "response variable into different sources of variation. The three types of sums of squares, namely Type I, Type II, and Type III,\n",
        "differ in how they allocate and account for the variation in the model.\n",
        "Here's a brief explanation of each:\n",
        "\n",
        "Type I Sums of Squares:\n",
        "\n",
        "Type I SS, also known as sequential or hierarchical SS, is calculated by sequentially adding predictors to the model in a specific order.\n",
        "The order of predictor addition is determined by the specified order of variables in the model formula.\n",
        "\n",
        "\n",
        "10. Explain the concept of deviance in a GLM.\n",
        "In a Generalized Linear Model (GLM), deviance is a measure of the goodness-of-fit of the model to the data.\n",
        "It quantifies the discrepancy between the observed data and the fitted model, similar to the concept of residuals in linear regression."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5P0NIYJ-b40H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Regression:\n",
        "\n",
        "11. What is regression analysis and what is its purpose?\n",
        "To predict the continous output/depenent variable regression model is used\n",
        "\n",
        "12. What is the difference between simple linear regression and multiple linear regression?\n",
        "when there is one predictor then simple LR and when there are more than one predictor than multiple linear regression.\n",
        "\n",
        "13. How do you interpret the R-squared value in regression?\n",
        "1-rss/tss\n",
        "where rss is the sum of the difference from points from actual line\n",
        "and tss is sum of the difference of the points from mean line\n",
        "\n",
        "14. What is the difference between correlation and regression?\n",
        "how two variables are related to each other only if one increaes than other decreases else viceversa but in\n",
        "regression there is a consant term also plus the error if present.\n",
        "Dependency:\n",
        "\n",
        "Correlation: Correlation examines the relationship between two variables\n",
        "without designating one as the dependent variable and the other as independent.\n",
        "It analyzes their mutual association.\n",
        "\n",
        "15. What is the difference between the coefficients and the intercept in regression?\n",
        "Coefficients:\n",
        "\n",
        "The coefficients, also known as regression coefficients or slope coefficients, quantify the relationship between the predictor variables and the\n",
        "dependent variable.\n",
        "Each predictor variable in the regression equation has an associated coefficient, which represents the change in the dependent variable for a\n",
        "one-unit change in the corresponding predictor variable, while holding all other predictors constant.\n",
        "\n",
        "Intercept:\n",
        "\n",
        "The intercept, also known as the constant term or the y-intercept, is the value of the dependent variable when all predictor variables\n",
        "are set to zero.\n",
        "The intercept represents the starting point or the baseline value of the dependent variable when all predictors have no influence.\n",
        "In linear regression, the intercept is the value of the dependent variable when all predictor variables have zero impact, which may or may not\n",
        "have practical meaning depending on the context.\n",
        "The intercept is particularly relevant when the predictor variables are centered around zero or when the model includes categorical predictors.\n",
        "\n",
        "16. How do you handle outliers in regression analysis?\n",
        "Consider Transformation:\n",
        "\n",
        "If the outliers are skewing the data distribution or violating assumptions, consider applying transformations to the variables,\n",
        "such as logarithmic or power transformations.\n",
        "Transforming the variables can help mitigate the impact of outliers and make the data more suitable for regression analysis.\n",
        "\n",
        "17. What is the difference between ridge regression and ordinary least squares regression?\n",
        "ridge where we pnalize our loss function by |lambda| square value to reduce overfitting\n",
        "\n",
        "18. What is heteroscedasticity in regression and how does it affect the model?\n",
        "when spreading of error is not constant across all predictor variables\n",
        "by this we cannot reduce error to approx zero value\n",
        "\n",
        "19. How do you handle multicollinearity in regression analysis?\n",
        "by removing features or extracting more data\n",
        "\n",
        "20. What is polynomial regression and when is it used?\n",
        "Polynomial regression is a form of regression analysis that models the relationship between the dependent variable and the\n",
        "independent variable(s) as an nth-degree polynomial. Unlike simple linear regression, which assumes a linear relationship, polynomial regression\n",
        "allows for curved or nonlinear relationships between the variables."
      ],
      "metadata": {
        "id": "1KbrkGYbb46B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NQDzXBjYb49G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Loss function:\n",
        "\n",
        "21. What is a loss function and what is its purpose in machine learning?\n",
        "loss funtion- mse\n",
        "it is used to reduce the error between dependent and independent variable\n",
        "\n",
        "22. What is the difference between a convex and non-convex loss function?\n",
        "convex when squared function is used and its derivative can be calculated\n",
        "non-convex - mae means its derivative is constant across points.\n",
        "\n",
        "23. What is mean squared error (MSE) and how is it calculated?\n",
        "mse = (y-y*)*2\n",
        "squared differences between actual and predicted outcome variables.\n",
        "\n",
        "24. What is mean absolute error (MAE) and how is it calculated?\n",
        "mae= |y-y*|\n",
        "it gives the prediction error\n",
        "\n",
        "25. What is log loss (cross-entropy loss) and how is it calculated?\n",
        "Log loss, also known as cross-entropy loss or logistic loss, is a commonly used loss function in classification problems.\n",
        "It measures the discrepancy between predicted probabilities and true class labels.\n",
        "Log loss is particularly suitable for binary classification tasks, where there are two classes (e.g., classifying emails as spam or not spam).\n",
        "\n",
        "26. How do you choose the appropriate loss function for a given problem?\n",
        "for large error uses mse for small errorif you treat all error equally use mae\n",
        "\n",
        "27. Explain the concept of regularization in the context of loss functions.\n",
        "lasso for feature selection by penalizing the error by adding lambdaa to loss function , ridge for reducing overfitting by adding\n",
        "squarred lambdaa to loss function\n",
        "\n",
        "28. What is Huber loss and how does it handle outliers?\n",
        "29. What is quantile loss and when is it used?\n",
        "30. What is the difference between squared loss and absolute loss?\n",
        "\n",
        "mse= (y-y_pred)*2\n",
        "mae= abs(y-y_pred)"
      ],
      "metadata": {
        "id": "eLAlZk1gb4_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ee2OmWHzb5Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Optimizer (GD):\n",
        "\n",
        "31. What is an optimizer and what is its purpose in machine learning?\n",
        "to preduce the errors\n",
        "error = (y-y_pred)*2\n",
        "optimizer--> de/dm\n",
        "de/dc\n",
        "where y_pred= mx+c\n",
        "for linear model .\n",
        "\n",
        "32. What is Gradient Descent (GD) and how does it work?\n",
        "It is an optimizer for optimizing the error it is used\n",
        "\n",
        "33. What are the different variations of Gradient Descent?\n",
        "Batch Gradient Descent: In batch gradient descent, the algorithm calculates the gradient of the loss function using the entire training\n",
        "dataset in each iteration. It updates the model parameters based on the average gradient over all training examples. Batch\n",
        "gradient descent can be computationally expensive for large datasets but often provides more stable convergence.\n",
        "\n",
        "Stochastic Gradient Descent (SGD): In SGD, the algorithm randomly selects a single training example or a mini-batch of training\n",
        "examples in each iteration to compute the gradient and update the model parameters. SGD is computationally efficient and works well for large\n",
        "datasets. It introduces more noise in the optimization process but can converge faster and handle non-convex optimization problems.\n",
        "\n",
        "34. What is the learning rate in GD and how do you choose an appropriate value?\n",
        "always taken in range of 0 to 1\n",
        "always taken low so to reach global minima\n",
        "\n",
        "35. How does GD handle local optima in optimization problems?\n",
        "by learning rate optimized values\n",
        "\n",
        "36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?\n",
        "Stochastic Gradient Descent (SGD): In SGD, the algorithm randomly selects a single training example or a mini-batch of training\n",
        "examples in each iteration to compute the gradient and update the model parameters. SGD is computationally efficient\n",
        "and works well for large datasets.It introduces more noise in the optimization process\n",
        "but can converge faster and handle non-convex optimization problems.\n",
        "\n",
        "37. Explain the concept of batch size in GD and its impact on training.\n",
        "\n",
        "38. What is the role of momentum in optimization algorithms?\n",
        "To get awaay from local minima\n",
        "\n",
        "39. What is the difference between batch GD, mini-batch GD, and SGD?\n",
        "Batch Gradient Descent: In batch gradient descent, the algorithm calculates the gradient of the loss function\n",
        "using the entire training dataset in each iteration. It updates the model parameters based on the average gradient\n",
        "over all training examples. Batch gradient descent can be computationally expensive for large datasets but often provides more stable convergence.\n",
        "\n",
        "Stochastic Gradient Descent (SGD): In SGD, the algorithm randomly selects a single training example or a mini-batch of\n",
        "training examples in each iteration to compute the gradient and update the model parameters. SGD is computationally efficient and works well for\n",
        "large datasets. It introduces more noise in the optimization process but can converge faster and handle non-convex optimization problems.\n",
        "\n",
        "Mini-Batch Gradient Descent: Mini-batch gradient descent is a compromise between batch gradient descent and SGD.\n",
        "It randomly selects a small mini-batch of training examples (typically between 10 and 1,000) in each iteration.\n",
        "It combines the benefits of both batch gradient descent (more stable convergence) and SGD (computational efficiency and noise reduction .\n",
        "\n",
        "40. How does the learning rate affect the convergence of GD?\n",
        "It helps in reaching global minima."
      ],
      "metadata": {
        "id": "KHfIf6fJb5FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cumrAT9Rb5HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Regularization:\n",
        "\n",
        "41. What is regularization and why is it used in machine learning?\n",
        "it is used to make our model a stable generalized model\n",
        "uses- to reduce overfiiting and for feature selection\n",
        "\n",
        "42. What is the difference between L1 and L2 regularization?\n",
        "L1 adds only lambda and slope to loss function , useful in feature selection\n",
        "L2 add lambda and slope square to loss function , it is useful to reduce overfitting\n",
        "\n",
        "43. Explain the concept of ridge regression and its role in regularization.\n",
        "L2 add lambda square to loss function , it is useful to reduce overfitting\n",
        "output= loss_function + lambda and slope square\n",
        "\n",
        "44. What is the elastic net regularization and how does it combine L1 and L2 penalties?\n",
        "it is the summtion of both l1 and l2\n",
        "Elastic Net Regularization = α * L1 Penalty + (1 - α) * L2 Penalty\n",
        "\n",
        "The L1 penalty, also known as the Lasso penalty, is calculated as the sum of the absolute values of the coefficients:\n",
        "\n",
        "L1 Penalty = λ * ||β||₁\n",
        "\n",
        "Here, λ (lambda) is the regularization parameter that controls the strength of the penalty, and ||β||₁\n",
        "represents the L1 norm of the coefficient vector β.\n",
        "\n",
        "The L2 penalty, also known as the Ridge penalty, is calculated as the sum of the squared values of the coefficients:\n",
        "\n",
        "L2 Penalty = λ * ||β||₂²\n",
        "\n",
        "\n",
        "45. How does regularization help prevent overfitting in machine learning models?\n",
        "By adding lambda *(slope)*2 to our loss function this means when actual and predicted values become zero but in that case test accuracy might be\n",
        "low so that training accuracy does not become that more , a slope square is adding to it.\n",
        "\n",
        "46. What is early stopping and how does it relate to regularization?\n",
        "early stopping also prevents overfitting by slecting only those values and params which are required.\n",
        "\n",
        "47. Explain the concept of dropout regularization in neural networks.\n",
        "dropout layers are dropped out of the networks\n",
        "\n",
        "48. How do you choose the regularization parameter in a model?\n",
        "for feature selction use lasso , for reducing overfitting use L2 (Ridge regression)\n",
        "49. What is the difference between feature selection and regularization?\n",
        "feature selection simply means to select that features only which our of atmost importance\n",
        "but in general regularization means to tabilize out model , to generalize out model\n",
        "\n",
        "50. What is the trade-off between bias and variance in regularized models?\n",
        "In regularized models, there is a trade-off between bias and variance. Bias refers to the error introduced by approximating\n",
        "a real-world problem with a\n",
        " simplified model, while variance refers to the sensitivity of the model's performance to fluctuations in the training data.\n",
        "\n",
        "Regularization techniques, such as L1 regularization (Lasso) and L2 regularization (Ridge), introduce a penalty\n",
        "term to the loss function that helps control the complexity of the model. This penalty term adds a bias to the model's estimates, which\n",
        "can reduce the variance but increase the bias."
      ],
      "metadata": {
        "id": "52cikaAEb5K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EEx_G3maloHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM:\n",
        "\n",
        "51. What is Support Vector Machines (SVM) and how does it work?\n",
        "svm are used when data is not linearly separable , so tat using the kenrnel function it converts the adta\n",
        "to a higher plane and then draw a hyperplane and make predictions or separate the data into two sides of the hyperplane\n",
        "to categorize the data into two classess.\n",
        "\n",
        "52. How does the kernel trick work in SVM?\n",
        "using polynomial kernel or x squared kernel it separates the data by converting data to one higher dimensional\n",
        "\n",
        "53. What are support vectors in SVM and why are they important?\n",
        "these are vectors that are present on the boundaries of the hyperplane they are used for making predictions for the\n",
        "other data present near to them\n",
        "\n",
        "54. Explain the concept of the margin in SVM and its impact on model performance.\n",
        "margin is the line that is drwan to separate two kind of the data.\n",
        "\n",
        "55. How do you handle unbalanced datasets in SVM?\n",
        "56. What is the difference between linear SVM and non-linear SVM?\n",
        "non-linear uses kernel\n",
        "\n",
        "57. What is the role of C-parameter in SVM and how does it affect the decision boundary?\n",
        "C is a regularization parameter in SVM that controls the trade-off between achieving a large margin and minimizing misclassifications.\n",
        "A small C allows for a larger margin\n",
        "but may result in more misclassifications, while a large C aims to minimize misclassifications but may result in a smaller margin.\n",
        "\n",
        "58. Explain the concept of slack variables in SVM.\n",
        "59. What is the difference between hard margin and soft margin in SVM?\n",
        "60. How do you interpret the coefficients in an SVM model?"
      ],
      "metadata": {
        "id": "wVq_BMJ4loKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rKjxVrYloMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "61. What is a decision tree and how does it work?\n",
        "using gini or entropy to predict feature of higher importance and then including several other features on its branches to build a complete tree\n",
        "for regresson we calculate reduction in variance = variance of root - varinace of left and right child\n",
        "\n",
        "62. How do you make splits in a decision tree?\n",
        "by gini\n",
        "n.of instance *summation 1-p(yes)-p(no) / total instances\n",
        "\n",
        "for regresson we calculate reduction in variance = variance of root - varinace of left and right child\n",
        "\n",
        "63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?\n",
        "by gini\n",
        "n.of instance *summation 1-p(yes)-p(no) / total instances\n",
        "by entropy\n",
        "no. of instance * (-)p(yes)log(p(yes))(-)p(no)log(p(no)) /total instances\n",
        "\n",
        "64. Explain the concept of information gain in decision trees.\n",
        "It gives the measure of the importnace of the feature\n",
        "IG = entropy(output) - entropy(features)\n",
        "\n",
        "By entropy\n",
        "no. of instance * (-)p(yes)log(p(yes))(-)p(no)log(p(no)) /total instances\n",
        "\n",
        "65. How do you handle missing values in decision trees?\n",
        "Missing value imputation:\n",
        "\n",
        "Fill in missing values with estimated values based on statistical methods such as mean, median, mode, or regression imputation.\n",
        "Imputing missing values ensures that all data points have complete information,\n",
        "allowing the decision tree to make decisions based on the available data.\n",
        "66. What is pruning in decision trees and why is it important?\n",
        "it is doen to reduce overfitting , simply by deleting the tree which is not that necessary\n",
        "\n",
        "67. What is the difference between a classification tree and a regression tree?\n",
        "it make splits based on p(yes) and then p(no)\n",
        "for regresson we calculate reduction in variance = variance of root - varinace of left and right child\n",
        "\n",
        "68. How do you interpret the decision boundaries in a decision tree?\n",
        "for regresson we calculate reduction in variance = variance of root - varinace of left and right child\n",
        "\n",
        "69. What is the role of feature importance in decision trees?\n",
        "only important feature will be considered root and any other importnace feature will not get dropped un necessarily in pruning.\n",
        "\n",
        "70. What are ensemble techniques and how are they related to decision trees?\n",
        "To combine several weak trees to predict a final output\n",
        "bagging, boosting , voting and stacking consitute Ensemble."
      ],
      "metadata": {
        "id": "Umwdu72cloQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5PU9TUFUlomp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ensemble Techniques:\n",
        "\n",
        "71. What are ensemble techniques in machine learning?\n",
        "To combine several weak trees to predict a final output\n",
        "bagging, boosting , voting and stacking consitute Ensemble.\n",
        "\n",
        "72. What is bagging and how is it used in ensemble learning?\n",
        "to predict from several range of predictors and then based on majority making the decisions for classification\n",
        "and averaging for regression based tasks.\n",
        "\n",
        "73. Explain the concept of bootstrapping in bagging.\n",
        "Bootstrapping is a statistical technique used in bagging (Bootstrap Aggregation) to create subsets of data for training\n",
        "individual models in an ensemble.\n",
        "It involves randomly sampling the original dataset with replacement to generate multiple bootstrap samples.\n",
        "\n",
        "74. What is boosting and how does it work?\n",
        "boosting means to update the weights simply by applying weightage to each of the predictors based from low to high.\n",
        "\n",
        "75. What is the difference between AdaBoost and Gradient Boosting?\n",
        "Adaboost uses single DT with max depth of 1 and assign equal weightage to all of the trees first.\n",
        "Gradient boost simply supplies the input to second decision tree from prediction of the first Decision tree.\n",
        "\n",
        "76. What is the purpose of random forests in ensemble learning?\n",
        "Bagging approach means several DT are made using sampling with replacement and then by averaging their predictions\n",
        "the output is predicted for regression or by majority voting for classifier.\n",
        "\n",
        "77. How do random forests handle feature importance?\n",
        "By using Gini index.\n",
        "\n",
        "78. What is stacking in ensemble learning and how does it work?\n",
        "stacking means to use several models for making the predictions like predictions from two models are given as input to another model\n",
        "and then prediction is made from them\n",
        "\n",
        "79. What are the advantages and disadvantages of ensemble techniques?\n",
        "advantages- Not need to do scaling , generally gets good accuracy\n",
        "disadvantages- Overfitting\n",
        "\n",
        "80. How do you choose the optimal number of models in an ensemble?\n",
        "Cross-Validation: One approach is to use cross-validation techniques, such as k-fold cross-validation or nested\n",
        "cross-validation, to evaluate the performance of the ensemble with different numbers of models. By training and evaluating\n",
        "the ensemble on multiple folds or iterations, you can observe how the performance stabilizes or reaches a\n",
        "plateau with increasing model count. You can then choose the number of models that achieves the best trade-off between performance and complexity."
      ],
      "metadata": {
        "id": "CftmBnQ984Sd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
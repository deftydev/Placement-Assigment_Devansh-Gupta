{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwYsXbihbgTn"
      },
      "outputs": [],
      "source": [
        "Naive Approach:\n",
        "\n",
        "1. What is the Naive Approach in machine learning?\n",
        "naive approach in ML is to refer to the naive bayes theorem where conditional probability is used for determining of the output .\n",
        "\n",
        "2. Explain the assumptions of feature independence in the Naive Approach.\n",
        "In this the features should be independent of each other, as one feature should not influence the other feature.\n",
        "\n",
        "3. How does the Naive Approach handle missing values in the data?\n",
        "he Naive Approach handles missing values by either ignoring the instance with missing\n",
        "values during training and classification or by considering missing values as a separate category during the probability estimation.\n",
        "\n",
        "4. What are the advantages and disadvantages of the Naive Approach?\n",
        "advantages- It is useful when ocurrence of one event depends on several events and you can create a model when it is depending on one or more\n",
        "than one feature\n",
        "disadvantage - It ignores the missing dataset.\n",
        "\n",
        "5. Can the Naive Approach be used for regression problems? If yes, how?\n",
        "By using the normal distribution\n",
        "\n",
        "6. How do you handle categorical features in the Naive Approach?\n",
        "By using one hot encoding or label encoding\n",
        "\n",
        "7. What is Laplace smoothing and why is it used in the Naive Approach?\n",
        "Laplace smoothing: Laplace smoothing, also known as additive smoothing, is a technique used in the Naive Approach to handle the issue\n",
        "of zero probabilities.\n",
        "It involves adding a small constant (typically 1) to the counts of each feature to ensure that no feature probability becomes zero.\n",
        "\n",
        "8. How do you choose the appropriate probability threshold in the Naive Approach?\n",
        "Choosing the probability threshold: The choice of the probability threshold depends on the specific requirements of the\n",
        "classification problem and the desired trade-off between precision and recall.\n",
        "By adjusting the threshold, one can control the balance between false positives and false negatives in the predictions.\n",
        "\n",
        "9. Give an example scenario where the Naive Approach can be applied.\n",
        "The Naive Approach can be applied in various text classification tasks, such as spam detection, sentiment analysis, or topic classification."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cBPFXd6MlgNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KNN:\n",
        "\n",
        "10. What is the K-Nearest Neighbors (KNN) algorithm?\n",
        "KNN algorithm calulate the euclidean distanc between the data points and the distance to which the predicted point is nearer\n",
        "to that then it determines k value it selects k neighbours to which this point is closest and the based\n",
        "on thier probability of the output classes of those points which class has got the highest probability the output class of\n",
        "newlypredicted point is that class only .\n",
        "\n",
        "EUD= np.sqrt(X2**X2 - X1**X1)\n",
        "\n",
        "11. How does the KNN algorithm work?\n",
        "KNN algorithm calulate the euclidean distanc between the data points and the distance to which the predicted point is nearer\n",
        "to that then it determines k value it selects k neighbours to which this point is closest and the based\n",
        "on thier probability of the output classes of those points which class has got the highest probability the output class of\n",
        "newlypredicted point is that class only .\n",
        "\n",
        "EUD= np.sqrt(X2**X2 - X1**X1)\n",
        "\n",
        "12. How do you choose the value of K in KNN?\n",
        "Rule of Thumb: A common rule of thumb is to choose K as the square root of the total number of data points in the training set.\n",
        " For example, if you have 100 data points,\n",
        "you might start with K=10 (since âˆš100 = 10). This rule provides a good starting point but may not\n",
        "always be optimal for your specific dataset.\n",
        "\n",
        "13. What are the advantages and disadvantages of the KNN algorithm?\n",
        "Advantages of the K-Nearest Neighbors (KNN) algorithm:\n",
        "\n",
        "Simplicity: KNN is a simple and intuitive algorithm. It is easy to understand\n",
        "and implement, making it accessible to beginners and non-experts in machine learning.\n",
        "\n",
        "No Training Phase: KNN is an instance-based learning algorithm, which means it does not require a training phase.\n",
        "It simply stores the training data in memory, making the prediction phase computationally efficient.\n",
        "\n",
        "Disadvantages-: 1- Lazy learner\n",
        "2- Not recommended for categorical datasets and imbalanced datasets\n",
        "\n",
        "14. How does the choice of distance metric affect the performance of KNN?\n",
        "Euclidean Distance: Euclidean distance is the most commonly used distance metric in KNN.\n",
        "It measures the straight-line distance between two points in Euclidean space. Euclidean distance works well when the dataset has continuous\n",
        " numerical features and the assumption of feature independence holds.\n",
        " However, it can be sensitive to the scale of the features, so it is essential to normalize or standardize the features before applying KNN.\n",
        "\n",
        "Manhattan Distance: Manhattan distance, also known as city block distance or L1 distance,\n",
        "measures the sum of absolute differences between the coordinates of two points. It is suitable when\n",
        "dealing with features that are not continuous or have different scales. Manhattan distance is less sensitive to outliers\n",
        "compared to Euclidean distance and can be useful in cases where the feature scales or units vary significantly.\n",
        "\n",
        "15. Can KNN handle imbalanced datasets? If yes, how?\n",
        "Resampling Techniques: Resampling techniques can help rebalance the class distribution in the training dataset.\n",
        "Oversampling involves increasing the number of instances in the minority class, while undersampling involves\n",
        " reducing the number of instances in the majority class. These techniques aim to create a more balanced representation of the classes, which\n",
        "  can improve the performance of KNN in predicting minority class instances.\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a popular oversampling technique specifically designed for imbalanced datasets.\n",
        "It generates synthetic examples for the minority class by interpolating between existing instances. This helps to increase the\n",
        "diversity of the minority class and reduce the bias towards the majority class in KNN\n",
        "\n",
        "16. How do you handle categorical features in KNN?\n",
        "By calculating humming distances between the variables\n",
        "\n",
        "17. What are some techniques for improving the efficiency of KNN?\n",
        "To improve the efficiency of KNN you should find an optimal value of K.\n",
        "Your data should be well classified\n",
        "Dimensionality Reduction: If your dataset has high dimensionality, consider applying dimensionality reduction techniques\n",
        "such as Principal Component Analysis (PCA) or t-SNE to reduce the number of features while retaining the most important information.\n",
        "By reducing the dimensionality, you can potentially improve the efficiency of KNN by reducing the\n",
        "computation required for distance calculations.\n",
        "\n",
        "18. Give an example scenario where KNN can be applied.\n",
        "Suppose you have to classify people smoking habits based on their spending on ciggartes."
      ],
      "metadata": {
        "id": "cXXE5na5lgQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GsZbMn_8lgSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Clustering:\n",
        "\n",
        "19. What is clustering in machine learning?\n",
        "Clustering mend to group similar kind of data and create a separate cluster for each of them and then when any new data points comes\n",
        "it will be mapped to the nearest cluster by using any clustering algorithms , eg= K-means++, heirarchical or DBSCAN\n",
        "\n",
        "20. Explain the difference between hierarchical clustering and k-means clustering.\n",
        "In K-means clustering the optimal value of K is taken by plotting wcss(within cluster sum of squares value) verses no. of K known as elbow\n",
        "plot and then after finding the value from the screen plot here wcss is kind of becoming contact throughout that value is taken as\n",
        "K value\n",
        "After that random three data points are drwan as defind by K and then the data points nearer to them are mapped by euclidean distance\n",
        "But for heirarchical clustering firstly every single data point form its individual cluster then the cluster will be formed in the groups of two\n",
        "based on euclidean distance after that in similar kind of pattern all groups are formed and the longest vertical line is taken\n",
        "and then horixontal line is drwan through it and whichever number of vertical line it is cutting through that number is taken as value of k.\n",
        "\n",
        "21. How do you determine the optimal number of clusters in k-means clustering?\n",
        "In K-means clustering the optimal value of K is taken by plotting wcss(within cluster sum of squares value) verses no. of K known as screen\n",
        "plot and then after finding the value from the screen plot here wcss is kind of becoming contact throughout that value is taken as\n",
        "K value\n",
        "\n",
        "22. What are some common distance metrics used in clustering?\n",
        "Euclidean Distance: Euclidean distance is the most commonly used distance metric in KNN.\n",
        "It measures the straight-line distance between two points in Euclidean space. Euclidean distance works well when the dataset has continuous\n",
        " numerical features and the assumption of feature independence holds.\n",
        " However, it can be sensitive to the scale of the features, so it is essential to normalize or standardize the features before applying KNN.\n",
        "\n",
        "Manhattan Distance: Manhattan distance, also known as city block distance or L1 distance,\n",
        "measures the sum of absolute differences between the coordinates of two points. It is suitable when\n",
        "dealing with features that are not continuous or have different scales. Manhattan distance is less sensitive to outliers\n",
        "compared to Euclidean distance and can be useful in cases where the feature scales or units vary significantly.\n",
        "\n",
        "23. How do you handle categorical features in clustering?\n",
        "Using humming distance between the variables\n",
        "\n",
        "24. What are the advantages and disadvantages of hierarchical clustering?\n",
        "Advantages- Suitable for large datasets, simple and intuitive\n",
        "Disadvantages- If data points are placed next to each other then low accuracy we will get, not suitable for categorical datasets\n",
        "\n",
        "25. Explain the concept of silhouette score and its interpretation in clustering.\n",
        "It determines how your data is well separated.\n",
        "Silhoutte score= b(x)-a(x) // max(a(x),b(x))\n",
        "if your silhoutte score is large then it means your data is well separated\n",
        "a(x)= distance of the x points within other points of same cluster\n",
        "b(x)= distance of the x points with the other points outside of cluster\n",
        "\n",
        "26. Give an example scenario where clustering can be applied.\n",
        "For outlier detection, for data analysis, for grouping based scenario like grouping customer based on their spending scores"
      ],
      "metadata": {
        "id": "saFKjUl_lgVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WRsoGnEdlgXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dimension Reduction:\n",
        "\n",
        "34. What is dimension reduction in machine learning?\n",
        "Dimeansion reduction technique simply is introduced when you have 100-100's of features and\n",
        "your algorithm is not able to find patterns between the data and unable to classify\n",
        "similar kind of data and giving less accuracy so for that there comes\n",
        "dimensionality reduction.\n",
        "\n",
        "35. Explain the difference between feature selection and feature extraction.\n",
        "Feature slection means to select the feature based on its importance in the dataset like based on information gain\n",
        "Whereas Feature extraction means to extract the faetures from a feature and then based on extraction making the choices\n",
        "if that feature will is suitable to be taken into algorithm or not.\n",
        "\n",
        "36. How does Principal Component Analysis (PCA) work for dimension reduction?\n",
        "It just draws the line which covers all the points and then second PCA will be perpendicular to firstPCA\n",
        "After that using screen plot it will determine which no. of PCA is required.\n",
        "EUR= DISTANCE OF PC1 POINTS // DISTANCE OF PC1 + DISTANCE OF PC2\n",
        "\n",
        "37. How do you choose the number of components in PCA?\n",
        " using screen plot it will determine which no. of PCA is required.\n",
        "EUR= DISTANCE OF PC1 POINTS // DISTANCE OF PC1 + DISTANCE OF PC2\n",
        "EUR is plotted versus no. of PCA and when We get a constant line that means that much no. of PCA explained the variance\n",
        "well between the data provided.\n",
        "\n",
        "38. What are some other dimension reduction techniques besides PCA?\n",
        "t-SNE\n",
        "39. Give an example scenario where dimension reduction can be applied.\n",
        "When you're have 1000 of features and you dont know which no. of features wil be taken\n",
        "like in sensor fault detection several kind of sensors you have so you calculate PCA so\n",
        "that it well group the data using the screen plot which said that sing screen plot it will determine which no. of PCA is required.\n",
        "EUR= DISTANCE OF PC1 POINTS // DISTANCE OF PC1 + DISTANCE OF PC2\n",
        "EUR is plotted versus no. of PCA and when We get a constant line that means that much no. of PCA explained the variance\n",
        "well between the data provided."
      ],
      "metadata": {
        "id": "m_UI43Y8lgaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "js-owjt-lgcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Feature Selection:\n",
        "\n",
        "40. What is feature selection in machine learning?\n",
        "Feature slection means to select the feature based on its importance in the dataset like based on information gain\n",
        "Or by using L1 regulariztion.\n",
        "\n",
        "41. Explain the difference between filter, wrapper, and embedded methods of feature selection.\n",
        " filter methods assess feature relevance independently of the learning algorithm,\n",
        "  wrapper methods evaluate feature subsets by training and evaluating the learning algorithm, and embedded methods\n",
        "  integrate feature selection within the learning algorithm itself.\n",
        "\n",
        "42. How does correlation-based feature selection work?\n",
        "By using df.corr() and find corr coefficient and based on its value selecting the feature.\n",
        "\n",
        "43. How do you handle multicollinearity in feature selection?\n",
        "By droopping the features, extracting more data or by using PCA\n",
        "\n",
        "44. What are some common feature selection metrics?\n",
        "Correlation coeffiecient= Df.corr() higher values indicates most correlation with output variables\n",
        "PCA- to transform features from Higher to lower dimension and then using silhoutte score and Screen plot\n",
        "\n",
        "45. Give an example scenario where feature selection can be applied.\n",
        "When you have 100 of features , suppose using ustomer churn analysis"
      ],
      "metadata": {
        "id": "_X1hHTThlgfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbyYyhi9lgi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data Drift Detection:\n",
        "\n",
        "46. What is data drift in machine learning?\n",
        "Data drift means when suddendly your data changes its shape due to noise in he dataset\n",
        "\n",
        "47. Why is data drift detection important?\n",
        "To ensure normality of the dataset it is important so that correct predictions can be made\n",
        "\n",
        "48. Explain the difference between concept drift and feature drift.\n",
        "Concept drift refers to the phenomenon where the underlying concept or relationship between the input features and the target variable in a machine\n",
        "learning problem changes over time. In other words, the patterns, relationships, or distribution of the data evolve, leading to a shift in the\n",
        "predictive model's accuracy or performance. Concept drift can occur due to various reasons, such as changes in user behavior, external factors,\n",
        "or evolving trends. It poses a challenge to models that were trained on historical data because the assumptions made during training may no\n",
        "longer hold, resulting in a degradation of model performance. Detecting and adapting to concept drift is crucial to maintain the\n",
        "accuracy and relevance of the predictive model over time.\n",
        "\n",
        "Feature Drift:\n",
        "Feature drift, on the other hand, refers to changes in the input features themselves. It occurs when the characteristics or properties of the input\n",
        "features change over time while the underlying concept remains the same. Feature drift can happen due to various factors, such as sensor\n",
        "malfunction, changes in data collection processes, or shifts in the data source.\n",
        "\n",
        "49. What are some techniques used for detecting data drift?\n",
        "By Data monitoring using AWS\n",
        "By plotting separate clusters of the data and look for abnormality\n",
        "\n",
        "50. How can you handle data drift in a machine learning model?\n",
        "By simply performing outlier detection , data validation"
      ],
      "metadata": {
        "id": "b1rqgTZulgnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0yb5J0hmlgpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data Leakage:\n",
        "\n",
        "51. What is data leakage in machine learning?\n",
        "Data leakage simply means when your testing data knows about your training data beforehand , this will lead to wrong predictions\n",
        "on the new set of data obtained.\n",
        "\n",
        "52. Why is data leakage a concern?\n",
        "Wrong predictions on new set of data\n",
        "\n",
        "53. Explain the difference between target leakage and train-test contamination.\n",
        "54. How can you identify and prevent data leakage in a machine learning pipeline?\n",
        "By gettign the accuracy of the model on new data\n",
        "By using fit_transform on train data only and transformon the test data only.\n",
        "\n",
        "55. What are some common sources of data leakage?\n",
        "preprocessor fiiting stage can be the main stage of the leakage.\n",
        "\n",
        "56. Give an example scenario where data leakage can occur.\n",
        "suupose youre working in cedit card fraud detection and it also has timestamp in it as a feature\n",
        "so if your model learns the pattern based on timestamp then it will predict new data based on pattern identified from timestamp\n",
        "so you should take due care by understanding all features\n",
        "and to remove timestamp like features for better model behaviours to get good/precise/correct/fact(persent in data) based predictions only ."
      ],
      "metadata": {
        "id": "RpPjU2IUlgsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Cx1Oi0HYGnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cross Validation:\n",
        "\n",
        "57. What is cross-validation in machine learning?\n",
        "it is to simply train your model on separate datasets to build a generalize model.\n",
        "\n",
        "58. Why is cross-validation important?\n",
        "so that data leakage can be found , for getting a generalized model\n",
        "\n",
        "59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation.\n",
        "K-fold means to evaluate your model k times by dividing your dataset like sometimes some is test data sometimes another is train data\n",
        "so it gives a genralize model\n",
        "stratified k fold CV means that you divide your dataset k times and build models but you also divide in such a way that\n",
        "any dataset does not contain the imbalanced datasets.\n",
        "\n",
        "60. How do you interpret the cross-validation results?\n",
        "Evaluate Performance Metrics:\n",
        "Look at the performance metrics calculated for each fold of the cross-validation. Common metrics include accuracy, precision, recall\n",
        "F1-score, or mean squared error, depending on the type of problem (classification or regression). Calculate the average and standard deviation\n",
        "of these metrics across all folds."
      ],
      "metadata": {
        "id": "ZXk2LYFAYGqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-K22cfHYGzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "27. What is anomaly detection in machine learning?\n",
        "anamoly simply means outlier detection like detection of fradaulent card transactions.\n",
        "\n",
        "28. Explain the difference between supervised and unsupervised anomaly detection.\n",
        "supervised anamoly does contain the output attached to it like in ipl data which contains score of every over\n",
        "but unsupervised does not contain output for that DBSCAN clusstering, isolated tree can be used .\n",
        "\n",
        "29. What are some common techniques used for anomaly detection?\n",
        "DBSCAN CLUSSTERING\n",
        "LOCAL OUTLIER FACTOR- if distance of point is more than other nearby points and also density around that point is lass as compared\n",
        "to other nearby points than that point may be termed as an outlier.\n",
        "Isolated trees- by determining the isolation score\n",
        "\n",
        "30. How does the One-Class SVM algorithm work for anomaly detection?\n",
        "by seaparating the anomaly points in the dataset.\n",
        "\n",
        "31. How do you choose the appropriate threshold for anomaly detection?\n",
        "F1 score can be calculated for various threshold which give better F1 score that threshold can be taken\n",
        "\n",
        "32. How do you handle imbalanced datasets in anomaly detection?\n",
        "undersampling, oversampling, smote, tomek, boosting algorithms\n",
        "\n",
        "33. Give an example scenario where anomaly detection can be applied.\n",
        "credit card fradaulent transactions , health conditions will not be considered as anomaly."
      ],
      "metadata": {
        "id": "s5MVNDvtYG1O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}